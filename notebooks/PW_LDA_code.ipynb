{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f792fc",
   "metadata": {
    "id": "37f792fc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import dtale\n",
    "import pymongo\n",
    "import csv\n",
    "import scispacy\n",
    "import spacy\n",
    "from sklearn.utils import parallel_backend   \n",
    "nlp = spacy.load(\"en_core_sci_lg\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59051a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ee3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a57b45",
   "metadata": {},
   "source": [
    "## Base functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ad2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "\n",
    "    \"\"\"import data from MongoDB\"\"\"\n",
    "\n",
    "    myclient = pymongo.MongoClient(\"mongodb+srv://lucas-deepen:DSIqP935gtFobYc2@cluster0.ixkyxa7.mongodb.net/?retryWrites=true&w=majority\")\n",
    "    mydb = myclient[\"cleanpapers\"]\n",
    "    mycol = mydb[\"cleanedf\"]\n",
    "    mydoc = mycol.find({}, {\"_id\":1,\"articleTitle\":1,\"abstract\":1,\"pubDate\":1,\"affiliations\":1})\n",
    "\n",
    "    print('----------Data imported----------')\n",
    "\n",
    "    return mydoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2efc641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe(mydoc,length=132820):\n",
    "\n",
    "    \"\"\"convert mongodb data to dataframe (full = 132820 rows)\"\"\"\n",
    "    \n",
    "    # data to dataframe and limit length\n",
    "\n",
    "    df = pd.DataFrame(list(mydoc)).set_index(['_id'])\n",
    "\n",
    "    df = df[df.abstract != '.'].iloc[:length,:]\n",
    "\n",
    "    # extract year from the pubDate column\n",
    "\n",
    "    df['pubDate'] = df['pubDate'].str.extract(r'(\\d{4})')\n",
    "\n",
    "    print ('----------DataFrame created----------')\n",
    "\n",
    "    print (df.head(15))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f649db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "\n",
    "    \"\"\"cleaning function for the abstract\"\"\"\n",
    "    \n",
    "    # extract medical terms\n",
    "      \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    doc_string = \" \".join(str(a) for a in doc.ents)\n",
    "\n",
    "    # transform abtract words into lower case\n",
    "\n",
    "    words = doc_string.lower()\n",
    "\n",
    "    # remove punctuations\n",
    "\n",
    "    for punctuation in string.punctuation:\n",
    "\n",
    "        words = words.replace(punctuation,'')\n",
    "\n",
    "    # remove digits\n",
    "\n",
    "    words = ''.join(char for char in words if not char.isdigit())\n",
    "\n",
    "    # tokenize sentences\n",
    "\n",
    "    tokenized_text = word_tokenize(words)\n",
    "\n",
    "    # remove stop words\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "    tokenized_sentence_cleaned = [w for w in tokenized_text\n",
    "                                if not w in stop_words]\n",
    "\n",
    "    # standardize verbs\n",
    "\n",
    "    verb_lemmatized = [WordNetLemmatizer().lemmatize(word, pos = \"v\")\n",
    "            for word in tokenized_sentence_cleaned]\n",
    "\n",
    "    # standardize nouns\n",
    "\n",
    "    noun_lemmatized = [WordNetLemmatizer().lemmatize(word, pos = \"n\")  # n --> nouns\n",
    "            for word in verb_lemmatized]\n",
    "    \n",
    "    # only words longer than 3 charachters:\n",
    "    \n",
    "    length_3 = [ word for word in noun_lemmatized if len(word) > 3 ]\n",
    "    \n",
    "    # re-join list into sentence\n",
    "\n",
    "    cleaned_txt = \" \".join(length_3)\n",
    "\n",
    "    return cleaned_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffaa2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_ginsem(text):\n",
    "\n",
    "    \"\"\"cleaning function for the abstract\"\"\"\n",
    "    \n",
    "    # extract medical terms\n",
    "      \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    doc_string = \" \".join(str(a) for a in doc.ents)\n",
    "\n",
    "    # transform abtract words into lower case\n",
    "\n",
    "    words = doc_string.lower()\n",
    "\n",
    "    # remove punctuations\n",
    "\n",
    "    for punctuation in string.punctuation:\n",
    "\n",
    "        words = words.replace(punctuation,'')\n",
    "\n",
    "    # remove digits\n",
    "\n",
    "    words = ''.join(char for char in words if not char.isdigit())\n",
    "\n",
    "    # tokenize sentences\n",
    "\n",
    "    tokenized_text = word_tokenize(words)\n",
    "\n",
    "    # remove stop words\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "    tokenized_sentence_cleaned = [w for w in tokenized_text\n",
    "                                if not w in stop_words]\n",
    "\n",
    "    # standardize verbs\n",
    "\n",
    "    verb_lemmatized = [WordNetLemmatizer().lemmatize(word, pos = \"v\")\n",
    "            for word in tokenized_sentence_cleaned]\n",
    "\n",
    "    # standardize nouns\n",
    "\n",
    "    noun_lemmatized = [WordNetLemmatizer().lemmatize(word, pos = \"n\")  # n --> nouns\n",
    "            for word in verb_lemmatized]\n",
    "    \n",
    "    # only words longer than 3 charachters:\n",
    "    \n",
    "    length_3 = [ word for word in noun_lemmatized if len(word) > 3 ]\n",
    "    \n",
    "    return length_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa1193a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    \n",
    "    \"\"\"clean abstract\"\"\"\n",
    "\n",
    "    df_ = df.copy()\n",
    "\n",
    "    # apply clean function to abstracts\n",
    "\n",
    "    df_.abstract = df_.abstract.astype(str).apply(cleaning)\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dbb74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ginsem(df):\n",
    "    \n",
    "    \"\"\"clean abstract\"\"\"\n",
    "\n",
    "    df_ = df.copy()\n",
    "\n",
    "    # apply clean function to abstracts\n",
    "\n",
    "    df_.abstract = df_.abstract.astype(str).apply(cleaning_ginsem)\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce9e3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "\n",
    "    \"\"\"generate tokenized dataframe\"\"\"\n",
    "\n",
    "    # intitialize vectorizer model\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True,\n",
    "                                analyzer='word',\n",
    "                                stop_words='english',\n",
    "                                max_df=0.6,min_df=0.01)#,\n",
    "                                #max_features=10000)\n",
    "\n",
    "    # fit_transform abstract\n",
    "\n",
    "    tfidf_abstract = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "\n",
    "    # create data frame with columns names\n",
    "\n",
    "    weighted_words = pd.DataFrame(tfidf_abstract.toarray(),\n",
    "                columns = tfidf_vectorizer.get_feature_names(),index=df.index).round(2)\n",
    "\n",
    "    print ('----------Abstract tokenized----------')\n",
    "\n",
    "    print (weighted_words.head(15))\n",
    "\n",
    "    return weighted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3386143",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2686cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Data imported----------\n"
     ]
    }
   ],
   "source": [
    "data = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ff6c6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------DataFrame created----------\n",
      "                                                   abstract  \\\n",
      "_id                                                           \n",
      "34314384  Intracortical microelectrode arrays (MEA) can ...   \n",
      "33996894  Medulloblastoma is the most common malignant c...   \n",
      "33862118  Nod-like receptor family pyrin domain containi...   \n",
      "33691255  Mice with chronic cochlear implants can signif...   \n",
      "33332038  An Auditory Brainstem Implant (ABI) is a techn...   \n",
      "31201186  Tinnitus may have a very severe impact on the ...   \n",
      "35509538  Manufacturing of customized three-dimensional ...   \n",
      "35024600  Injectable hydrogel has the advantage to fill ...   \n",
      "34425566  The evaluation of the long-term stability of E...   \n",
      "33762926  Mitochondria are organelles responsible for bi...   \n",
      "33647494  Evolutions in cranioplasty have allowed for th...   \n",
      "33431445  A 42-year-old woman presented with fever, left...   \n",
      "33318954  An estimated 3.8 million traumatic brain injur...   \n",
      "33025785  Modern development of flexible electronics has...   \n",
      "35961580  Gut microbiota alterations might affect the de...   \n",
      "\n",
      "                                               articleTitle pubDate  \\\n",
      "_id                                                                   \n",
      "34314384  Neuropathological effects of chronically impla...    2021   \n",
      "33996894  Veliparib Is an Effective Radiosensitizing Age...    2021   \n",
      "33862118  The NLRP3-related inflammasome modulates pain ...    2021   \n",
      "33691255  Development of a chronically-implanted mouse m...    2021   \n",
      "33332038                            [Hearing without ears].    2020   \n",
      "31201186  An auditory brainstem implant for treatment of...    2019   \n",
      "35509538  Customized alloplastic cranioplasty of large b...    2022   \n",
      "35024600  Injectable hyaluronic acid hydrogel loaded wit...    2022   \n",
      "34425566  Long-term stability of the chronic epidural wi...    2021   \n",
      "33762926  Mitochondrial Behavior in Axon Degeneration an...    2021   \n",
      "33647494  Cranioplasty Using Customized 3-Dimensional-Pr...    2021   \n",
      "33431445  Skull base osteomyelitis with secondary cavern...    2021   \n",
      "33318954     An implantable helmet for studying repeat TBI.    2020   \n",
      "33025785  Conductive Hydrogel for a Photothermal-Respons...    2020   \n",
      "35961580  Microbiota-derived metabolite Indoles induced ...    2022   \n",
      "\n",
      "                                               affiliations  \n",
      "_id                                                          \n",
      "34314384  Wu Tsai Neurosciences Institute, Stanford Univ...  \n",
      "33996894  Wu Tsai Neurosciences Institute, Stanford Univ...  \n",
      "33862118  Wu Tsai Neurosciences Institute, Stanford Univ...  \n",
      "33691255  Wu Tsai Neurosciences Institute, Stanford Univ...  \n",
      "33332038  Wu Tsai Neurosciences Institute, Stanford Univ...  \n",
      "31201186  Wu Tsai Neurosciences Institute, Stanford Univ...  \n",
      "35509538  Department of Mechanical Engineering, American...  \n",
      "35024600  Department of Mechanical Engineering, American...  \n",
      "34425566  Department of Mechanical Engineering, American...  \n",
      "33762926  Department of Mechanical Engineering, American...  \n",
      "33647494  Department of Mechanical Engineering, American...  \n",
      "33431445  Department of Mechanical Engineering, American...  \n",
      "33318954  Department of Mechanical Engineering, American...  \n",
      "33025785  Department of Mechanical Engineering, American...  \n",
      "35961580  Institute of Pulmonary Disease, Guangzhou Ches...  \n"
     ]
    }
   ],
   "source": [
    "df = dataframe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e2f9db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_abstract = clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4594b2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Abstract tokenized----------\n",
      "          aberrant  ability  abnormal  abnormality  absence  access  \\\n",
      "_id                                                                   \n",
      "34314384       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "33996894       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "33862118       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "33691255       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "33332038       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "31201186       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "35509538       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "35024600       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "34425566       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "33762926       0.0      0.0      0.09          0.0      0.0     0.0   \n",
      "33647494       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "33431445       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "33318954       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "33025785       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "35961580       0.0      0.0      0.00          0.0      0.0     0.0   \n",
      "\n",
      "          accumulation  accuracy  accurate  acid  ...  wildtype  window  \\\n",
      "_id                                               ...                     \n",
      "34314384          0.00       0.0       0.0  0.00  ...       0.0     0.0   \n",
      "33996894          0.00       0.0       0.0  0.00  ...       0.0     0.0   \n",
      "33862118          0.00       0.0       0.0  0.00  ...       0.0     0.0   \n",
      "33691255          0.00       0.0       0.0  0.00  ...       0.0     0.0   \n",
      "33332038          0.00       0.0       0.0  0.00  ...       0.0     0.0   \n",
      "31201186          0.00       0.0       0.0  0.00  ...       0.0     0.0   \n",
      "35509538          0.00       0.0       0.0  0.00  ...       0.0     0.0   \n",
      "35024600          0.00       0.0       0.0  0.27  ...       0.0     0.0   \n",
      "34425566          0.00       0.0       0.0  0.00  ...       0.0     0.0   \n",
      "33762926          0.00       0.0       0.0  0.00  ...       0.0     0.0   \n",
      "33647494          0.00       0.0       0.0  0.00  ...       0.0     0.0   \n",
      "33431445          0.00       0.0       0.0  0.00  ...       0.0     0.0   \n",
      "33318954          0.00       0.0       0.0  0.00  ...       0.0     0.0   \n",
      "33025785          0.00       0.0       0.0  0.00  ...       0.0     0.0   \n",
      "35961580          0.17       0.0       0.0  0.30  ...       0.0     0.0   \n",
      "\n",
      "          woman  work  world  worsen  xray  year  young  younger  \n",
      "_id                                                               \n",
      "34314384   0.00   0.0   0.00     0.0   0.0  0.00    0.0      0.0  \n",
      "33996894   0.00   0.0   0.00     0.0   0.0  0.00    0.0      0.0  \n",
      "33862118   0.00   0.0   0.00     0.0   0.0  0.00    0.0      0.0  \n",
      "33691255   0.00   0.0   0.00     0.0   0.0  0.00    0.0      0.0  \n",
      "33332038   0.00   0.0   0.00     0.0   0.0  0.00    0.0      0.0  \n",
      "31201186   0.00   0.0   0.00     0.0   0.0  0.13    0.0      0.0  \n",
      "35509538   0.00   0.0   0.00     0.0   0.0  0.00    0.0      0.0  \n",
      "35024600   0.00   0.0   0.00     0.0   0.0  0.00    0.0      0.0  \n",
      "34425566   0.00   0.0   0.00     0.0   0.0  0.05    0.0      0.0  \n",
      "33762926   0.00   0.0   0.00     0.0   0.0  0.00    0.0      0.0  \n",
      "33647494   0.00   0.0   0.09     0.0   0.0  0.10    0.0      0.0  \n",
      "33431445   0.07   0.0   0.00     0.0   0.0  0.00    0.0      0.0  \n",
      "33318954   0.00   0.0   0.00     0.0   0.0  0.07    0.0      0.0  \n",
      "33025785   0.00   0.0   0.00     0.0   0.0  0.00    0.0      0.0  \n",
      "35961580   0.00   0.0   0.00     0.0   0.0  0.00    0.0      0.0  \n",
      "\n",
      "[15 rows x 1063 columns]\n"
     ]
    }
   ],
   "source": [
    "token = tokenize(clean_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56cd81b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aberrant</th>\n",
       "      <th>ability</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>abnormality</th>\n",
       "      <th>absence</th>\n",
       "      <th>access</th>\n",
       "      <th>accumulation</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accurate</th>\n",
       "      <th>acid</th>\n",
       "      <th>...</th>\n",
       "      <th>wildtype</th>\n",
       "      <th>window</th>\n",
       "      <th>woman</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worsen</th>\n",
       "      <th>xray</th>\n",
       "      <th>year</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34314384</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33996894</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33862118</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33691255</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33332038</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35519270</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35519265</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35511603</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35510871</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35505021</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132813 rows × 1063 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aberrant  ability  abnormal  abnormality  absence  access  \\\n",
       "_id                                                                   \n",
       "34314384       0.0      0.0       0.0          0.0      0.0     0.0   \n",
       "33996894       0.0      0.0       0.0          0.0      0.0     0.0   \n",
       "33862118       0.0      0.0       0.0          0.0      0.0     0.0   \n",
       "33691255       0.0      0.0       0.0          0.0      0.0     0.0   \n",
       "33332038       0.0      0.0       0.0          0.0      0.0     0.0   \n",
       "...            ...      ...       ...          ...      ...     ...   \n",
       "35519270       0.0      0.0       0.0          0.0      0.0     0.0   \n",
       "35519265       0.0      0.0       0.0          0.0      0.0     0.0   \n",
       "35511603       0.0      0.0       0.0          0.0      0.0     0.0   \n",
       "35510871       0.0      0.0       0.0          0.0      0.0     0.0   \n",
       "35505021       0.0      0.0       0.0          0.0      0.0     0.0   \n",
       "\n",
       "          accumulation  accuracy  accurate  acid  ...  wildtype  window  \\\n",
       "_id                                               ...                     \n",
       "34314384           0.0       0.0       0.0   0.0  ...       0.0     0.0   \n",
       "33996894           0.0       0.0       0.0   0.0  ...       0.0     0.0   \n",
       "33862118           0.0       0.0       0.0   0.0  ...       0.0     0.0   \n",
       "33691255           0.0       0.0       0.0   0.0  ...       0.0     0.0   \n",
       "33332038           0.0       0.0       0.0   0.0  ...       0.0     0.0   \n",
       "...                ...       ...       ...   ...  ...       ...     ...   \n",
       "35519270           0.0       0.0       0.0   0.0  ...       0.0     0.0   \n",
       "35519265           0.0       0.0       0.0   0.0  ...       0.0     0.0   \n",
       "35511603           0.0       0.0       0.0   0.0  ...       0.0     0.0   \n",
       "35510871           0.0       0.0       0.0   0.0  ...       0.0     0.0   \n",
       "35505021           0.0       0.0       0.0   0.0  ...       0.0     0.0   \n",
       "\n",
       "          woman  work  world  worsen  xray  year  young  younger  \n",
       "_id                                                               \n",
       "34314384    0.0   0.0    0.0     0.0   0.0   0.0    0.0      0.0  \n",
       "33996894    0.0   0.0    0.0     0.0   0.0   0.0    0.0      0.0  \n",
       "33862118    0.0   0.0    0.0     0.0   0.0   0.0    0.0      0.0  \n",
       "33691255    0.0   0.0    0.0     0.0   0.0   0.0    0.0      0.0  \n",
       "33332038    0.0   0.0    0.0     0.0   0.0   0.0    0.0      0.0  \n",
       "...         ...   ...    ...     ...   ...   ...    ...      ...  \n",
       "35519270    0.0   0.0    0.0     0.0   0.0   0.0    0.0      0.0  \n",
       "35519265    0.0   0.0    0.0     0.0   0.0   0.0    0.0      0.0  \n",
       "35511603    0.0   0.0    0.0     0.0   0.0   0.0    0.0      0.0  \n",
       "35510871    0.0   0.0    0.0     0.0   0.0   0.0    0.0      0.0  \n",
       "35505021    0.0   0.0    0.0     0.0   0.0   0.0    0.0      0.0  \n",
       "\n",
       "[132813 rows x 1063 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8436c2ea",
   "metadata": {},
   "source": [
    "## LDA ginsem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7e55d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import TfidfModel, LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14f8664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(clean_abstract.abstract.str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "244dbb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b04eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aeb7426",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c009b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus = tfidf[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d7506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/patrickwestermann/.pyenv/versions/3.8.13/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/patrickwestermann/.pyenv/versions/3.8.13/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/patrickwestermann/.pyenv/versions/3.8.13/lib/python3.8/multiprocessing/pool.py\", line 109, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/Users/patrickwestermann/.pyenv/versions/3.8.13/envs/DeepSearch/lib/python3.8/site-packages/gensim/models/ldamulticore.py\", line 346, in worker_e_step\n",
      "    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?\n",
      "  File \"/Users/patrickwestermann/.pyenv/versions/3.8.13/envs/DeepSearch/lib/python3.8/site-packages/gensim/models/ldamodel.py\", line 767, in do_estep\n",
      "    gamma, sstats = self.inference(chunk, collect_sstats=True)\n",
      "  File \"/Users/patrickwestermann/.pyenv/versions/3.8.13/envs/DeepSearch/lib/python3.8/site-packages/gensim/models/ldamodel.py\", line 696, in inference\n",
      "    if len(doc) > 0 and not isinstance(doc[0][0], integer_types):\n",
      "TypeError: 'int' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "lda_ginsem = LdaMulticore(tfidf_corpus, id2word=id2word, num_topics=10,workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b271b",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def LDA(token,n_components,max_iter):\n",
    "    \n",
    "    lda_model = LatentDirichletAllocation(n_components=n_components,max_iter=max_iter,n_jobs=-1,learning_method='online')\n",
    "    lda_model.fit(token)\n",
    "    \n",
    "    return lda_model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics(model,token,topwords):\n",
    "    \n",
    "    topic_mixture = pd.DataFrame(model.components_,columns = token.columns)\n",
    "    \n",
    "    n_components = topic_mixture.shape[0]\n",
    "    \n",
    "    for topic in range(n_components):\n",
    "        print('-'*10)\n",
    "        print(f\"For topic {topic}, here are the top {topwords} words with weights:\")\n",
    "        \n",
    "        topic_df = topic_mixture.iloc[topic].sort_values(ascending = False).head(topwords)\n",
    "        \n",
    "        print(round(topic_df,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939b336",
   "metadata": {},
   "source": [
    "## LDA 15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c9bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_15 = \"\"\n",
    "\n",
    "with parallel_backend(\"threading\"):\n",
    "    lda_15 = LDA(token,15,100)\n",
    "    \n",
    "lda_t_15 = lda_15.transform(token)\n",
    "\n",
    "lda_s_15 = pd.DataFrame(lda_15.components_,columns = token.columns)\n",
    "\n",
    "lda_s_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7755158",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics(lda_15,token,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1974d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_15 = pd.DataFrame(lda_t_15,index=df.index)\n",
    "score_15[[0]].sort_values(by=0,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c47986",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_15 = list(score_15.columns)\n",
    "lda_topic_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_15 = pd.DataFrame(score_15.idxmax(axis=1),columns=['Topic'])\n",
    "topic_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82061db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['20058907']['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a53541e",
   "metadata": {},
   "source": [
    "## LDA 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22260d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_10 = \"\"\n",
    "with parallel_backend(\"threading\"):\n",
    "    lda_10 = LDA(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce55323",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_t_10 = lda_10.transform(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2285be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_s_10 = pd.DataFrame(lda_10.components_,columns = token.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67acc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_s_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics(lda_10,token,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be31789",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_10 = pd.DataFrame(lda_t_10,index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_10 = list(score_10.columns)\n",
    "lda_topic_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_10[[5]].sort_values(by=5,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0962252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['33834437']['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447fcea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = pd.DataFrame(score_10.idxmax(axis=1),columns=['Topic'])\n",
    "topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6d2fa",
   "metadata": {},
   "source": [
    "## List of topics per LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_list(model,token,topwords):\n",
    "    \n",
    "    topic_mixture = pd.DataFrame(model.components_,columns = token.columns)\n",
    "    \n",
    "    n_components = topic_mixture.shape[0]\n",
    "    \n",
    "    topics = []\n",
    "    \n",
    "    for topic in range(n_components):\n",
    "        \n",
    "        topic_df = topic_mixture.iloc[topic].sort_values(ascending = False).head(topwords)\n",
    "        \n",
    "        topics.append(list(topic_df.index))\n",
    "        \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340f229",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from gensim import corpora\n",
    "\n",
    "topic_10_100 = topic_list(lda,token,100)\n",
    "topic_15_100 = topic_list(lda_15,token,100)\n",
    "num_topics = [10,15]\n",
    "LDA_models = {10:lda,15:lda_15}\n",
    "LDA_topics = {10:topic_10_100,15:topic_15_100}\n",
    "\n",
    "def jaccard_similarity(topic_1, topic_2):\n",
    "    \"\"\"\n",
    "    Derives the Jaccard similarity of two topics\n",
    "\n",
    "    Jaccard similarity:\n",
    "    - A statistic used for comparing the similarity and diversity of sample sets\n",
    "    - J(A,B) = (A ∩ B)/(A ∪ B)\n",
    "    - Goal is low Jaccard scores for coverage of the diverse elements\n",
    "    \"\"\"\n",
    "    intersection = set(topic_1).intersection(set(topic_2))\n",
    "    union = set(topic_1).union(set(topic_2))\n",
    "                    \n",
    "    return float(len(intersection))/float(len(union))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae527cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_stability = {}\n",
    "\n",
    "for i in range(0, len(num_topics)-1):\n",
    "    jaccard_sims = []\n",
    "    for t1, topic1 in enumerate(LDA_topics[num_topics[i]]): # pylint: disable=unused-variable\n",
    "        sims = []\n",
    "        print (topic1)\n",
    "        for t2, topic2 in enumerate(LDA_topics[num_topics[i+1]]): # pylint: disable=unused-variable\n",
    "            print (topic2)\n",
    "            print (jaccard_similarity(topic1, topic2))\n",
    "            sims.append(jaccard_similarity(topic1, topic2))    \n",
    "            \n",
    "        jaccard_sims.append(sims)    \n",
    "    \n",
    "    LDA_stability[num_topics[i]] = jaccard_sims\n",
    "                \n",
    "mean_stabilities = [np.array(LDA_stability[i]).mean() for i in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888317fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirichlet_dict = corpora.Dictionary([token])\n",
    "dirichlet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherences = [CoherenceModel(model=LDA_models[i], texts=corpus, dictionary=dirichlet_dict, coherence='c_v').get_coherence()\\\n",
    "              for i in num_topics[:-1]]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "lm_nlp_average_frequency_01.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
